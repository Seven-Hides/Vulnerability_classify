# -*- coding: utf-8 -*-

import pandas as pd
import jieba
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
import tf_idf
import info_gain

"""
函数说明：简单分词
Parameters:
     filename:数据文件
Returns:
     list_word_split：分词后的数据集列表
     category_labels: 文本标签列表
"""
def word_split(filename):
    read_data=pd.read_excel(filename)
    list_word_split=[]
    category_labels=[]
    for i in range(len(read_data)):
        row_data = read_data.iloc[i, 1]           # 读取单个漏洞描述文本
        list_row_data = list(jieba.cut(row_data)) # 对单个漏洞进行分词
        list_row_data=[x for x in list_row_data if x!=' '] #去除列表中的空格字符
        list_word_split.append(list_row_data)

        row_data_label=read_data.iloc[i,2]   #读取单个漏洞的类别标签
        category_labels.append(row_data_label) #将单个漏洞的类别标签加入列表
    return list_word_split, category_labels


"""
函数说明：词性还原
Parameters:
     list_words:数据列表
Returns:
     list_words_lemmatizer：词性还原后的数据集列表
"""
def word_lemmatizer(list_words):
    wordnet_lemmatizer = WordNetLemmatizer()
    #porter_stemmer = PorterStemmer()
    list_words_lemmatizer = []
    for word_list in list_words:
        lemmatizer_word = []
        for i in word_list:
            lemmatizer_word.append(wordnet_lemmatizer.lemmatize(i))
            #lemmatizer_word.append(porter_stemmer.stem(i))
        list_words_lemmatizer.append(lemmatizer_word)
    return list_words_lemmatizer


"""
函数说明：停用词过滤
Parameters:
     filename:停用词文件
     list_words_lemmatizer:词列表
Returns:
     list_filter_stopwords：停用词过滤后的词列表
"""
def stopwords_filter(filename,list_words_lemmatizer):
    list_filter_stopwords=[]  #声明一个停用词过滤后的词列表
    with open(filename,'r') as fr:
        stop_words=list(fr.read().split('\n')) #将停用词读取到列表里
        for i in range(len(list_words_lemmatizer)):
            word_list = []
            for j in list_words_lemmatizer[i]:
                #j=j.lower() #将词变为小写
                if j not in stop_words:
                    word_list.append(j.lower()) #加入词列表
            list_filter_stopwords.append(word_list)
        return list_filter_stopwords


"""
函数说明：文本向量化，标签向量化   one-hot编码
Parameters:
     feature_words:特征词集
     doc_words:文本列表
     doc_category_labels:文本类别标签
Returns:
     docvec_list:文本向量列表
     labelvec_list:标签向量列表
"""
def words2vec(feature_words,doc_words,doc_category_labels):
    #文本列表转向量列表
    docvec_list=[]
    for words in doc_words:
        docvec = [0] * len(feature_words)
        for j in words:
            if j in feature_words:
                docvec[feature_words.index(j)]=1
        docvec_list.append(docvec)
    #标签列表转向量列表
    labelvec_list = []
    labelset=list(set(doc_category_labels))
    for label in doc_category_labels:
        doclabel = [0] * len(labelset)
        doclabel[labelset.index(label)]=1
        labelvec_list.append(doclabel)
    return docvec_list,labelvec_list


"""
函数说明：将文本数据转换为可用信息增益处理的数据集 （文本向量化   one-hot编码）
Parameters:
     feature_words:特征词集
     doc_words:文本列表
     doc_category_labels:文本类别标签
Returns:
     docvec_label_list:文本向量列表
"""
def words2vec_label(feature_words,doc_words,doc_category_labels):
    #文本列表转向量列表
    docvec_label_list=[]
    for i in range(len(doc_words)):
        docvec=[0]*len(feature_words)
        for j in doc_words[i]:
            if j in feature_words:
                docvec[feature_words.index(j)]=1

        docvec.append(doc_category_labels[i])
        docvec_label_list.append(docvec)
    return docvec_label_list



if __name__=='__main__':
    list_word_split, category_labels=word_split('all_datasets_10000.xls') #获得每条文本的分词列表和标签列表
    print('分词成功')
    list_words_lemmatizer=word_lemmatizer(list_word_split)  #词性还原
    print('词性还原成功')
    list_filter_stopwords=stopwords_filter('stopwords.txt',list_words_lemmatizer) #获得停用词过滤后的列表
    print("停用词过滤成功")


    dict_feature_select=tf_idf.feature_select(list_filter_stopwords) #使用TF-IDF进行特征提取
    feaNum=len(dict_feature_select)   #求特征词字典的个数
    print(feaNum)
    print(dict_feature_select)

    features_vocabSet=[]
    for i in range(1024):     #选择前1024个词作为重要特征词集
        features_vocabSet.append(dict_feature_select[i][0])
    print('根据tf-idf词集提取成功')



    docvec=words2vec_label(features_vocabSet,list_word_split,category_labels) #数据集转换
    features_info=info_gain.chooseBestFeatureToSplit(docvec)  #计算词集的信息增益

    print(features_info)
    #获取词集
    feature_words=[]
    for i in range(len(features_vocabSet)):
        if features_info[i][1]!=0:
            feature_words.append(features_vocabSet[features_info[i][0]])
    print('根据信息增益提取特征词集成功')




    docvec, labelvec = words2vec(feature_words, list_word_split, category_labels)
    print('构建词向量成功')
   # print(features_vocabSet[469],features_vocabSet[476],features_vocabSet[1],features_vocabSet[57],features_vocabSet[269],
   #       features_vocabSet[28],features_vocabSet[8],features_vocabSet[3],features_vocabSet[361],features_vocabSet[584],
   # features_vocabSet[320],features_vocabSet[7],features_vocabSet[856],)
